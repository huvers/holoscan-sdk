#!/usr/bin/env python3
"""
SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""  # noqa: E501

import argparse
import atexit
import fcntl
import logging
import os
import pickle
from os.path import abspath, dirname, join
from pathlib import Path

import gxf.core
import yaml

try:
    from registry.core.version import GXF_CORE_COMPATIBLE_VERSION, REGISTRY_CORE_VERSION
except ImportError:
    GXF_CORE_COMPATIBLE_VERSION = "4.2.0"
    REGISTRY_CORE_VERSION = "1.1"

# Configure the logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

#######################################################################################
# Extension dependency caching support
#######################################################################################
LOCAL_CACHE_DB_PATH = abspath(join(dirname(__file__), "gxf_extension_cache.pickle"))
LOCAL_CACHE_DB_LOCK_PATH = abspath(join(dirname(__file__), "gxf_extension_cache.pickle.lock"))
LOCAL_CACHE_DB_LOCK_FILE = None  # Lock file for the cache db

# extension path to (name, uuid, version) map
global_ext_path_to_extinfo_map = {}
# extension path to extension dependencies map (set of extension paths)
global_ext_path_to_dep_paths_map = {}

#######################################################################################
# Manifest generation data
#######################################################################################
DEFAULT_HOLOSCAN_DISTRIBUTION = "ubuntu_22.04"
DEFAULT_HOLOSCAN_CUDA = "12.6"
DEFAULT_HOLOSCAN_NAMESPACE = "holoscan"
DEFAULT_HOLOSCAN_LICENSE_FILE = abspath(join(dirname(__file__), "..", "LICENSE.txt"))
DEFAULT_HOLOSCAN_URL = "github.com/nvidia-holoscan"
DEFAULT_HOLOSCAN_GIT_REPOSITORY = "github.com/nvidia-holoscan/holoscan-sdk"


MANIFEST_HEADER = f"""# This file is auto-generated by generate_gxf_manifest.py
#  GXF Core Version: {GXF_CORE_COMPATIBLE_VERSION}
#  Registry Core Version: {REGISTRY_CORE_VERSION}"""

MANIFEST_TEMPLATE = (
    MANIFEST_HEADER
    + """
name:{name}
extension_library:{extension_library}
uuid:{uuid}
version:{version}
license_file:{license_file}
url:{url}
git_repository:{git_repository}
labels:{labels}
badges:{badges}
priority:{priority}
platform:
  arch:{arch}
  os:{os}
  distribution:{distribution}
compute:
  cuda:{cuda}
  tensorrt:{tensorrt}
  cudnn:{cudnn}
  deepstream:{deepstream}
  triton:{triton}
  vpi:{vpi}
dependencies:{dependencies}
headers:{headers}
binaries:{binaries}
python_alias:{python_alias}
namespace:{namespace}
python_bindings:{python_bindings}
python_sources:{python_sources}
data:{data}
"""
)

DEFAULT_HOLOSCAN_MANIFEST_DATA = {
    "name": None,
    "extension_library": None,
    "uuid": None,
    "version": None,
    "license_file": DEFAULT_HOLOSCAN_LICENSE_FILE,
    "url": DEFAULT_HOLOSCAN_URL,
    "git_repository": DEFAULT_HOLOSCAN_GIT_REPOSITORY,
    "labels": ["holoscan"],
    "badges": [],
    "priority": 1,
    "arch": "x86_64",
    "os": "linux",
    "distribution": DEFAULT_HOLOSCAN_DISTRIBUTION,
    "cuda": DEFAULT_HOLOSCAN_CUDA,
    "tensorrt": None,
    "cudnn": None,
    "deepstream": None,
    "triton": None,
    "vpi": None,
    "dependencies": [],
    "headers": [],
    "binaries": [],
    "python_alias": None,
    "namespace": DEFAULT_HOLOSCAN_NAMESPACE,
    "python_bindings": [],
    "python_sources": [],
    "data": [],
}


def load_cache_db() -> None:
    """Import a local cache database with extension dependency information."""
    global global_ext_path_to_extinfo_map
    global global_ext_path_to_dep_paths_map
    global LOCAL_CACHE_DB_LOCK_FILE
    global LOCAL_CACHE_DB_PATH

    if not LOCAL_CACHE_DB_PATH.endswith(".pickle"):
        logger.warning(
            f"Specified database file {LOCAL_CACHE_DB_PATH} does not look like a pickle file"
        )

    # Lock the cache db file
    LOCAL_CACHE_DB_LOCK_FILE = open(LOCAL_CACHE_DB_LOCK_PATH, "w")  # noqa: SIM115
    fcntl.flock(LOCAL_CACHE_DB_LOCK_FILE, fcntl.LOCK_EX)

    if Path(LOCAL_CACHE_DB_PATH).exists():
        try:
            with open(LOCAL_CACHE_DB_PATH, "rb") as f:
                cache_dict = pickle.load(f)
                global_ext_path_to_extinfo_map = cache_dict["ext_path_to_extinfo_map"]
                global_ext_path_to_dep_paths_map = cache_dict["ext_path_to_dep_paths_map"]
        except Exception as e:
            logger.warning(f"Failed to load cache from {LOCAL_CACHE_DB_PATH}: {e}")
            global_ext_path_to_extinfo_map = {}
            global_ext_path_to_dep_paths_map = {}
    else:
        global_ext_path_to_extinfo_map = {}
        global_ext_path_to_dep_paths_map = {}


def save_cache_db() -> None:
    """Export a local cache database with extension dependency information."""
    global LOCAL_CACHE_DB_LOCK_FILE

    try:
        with open(LOCAL_CACHE_DB_PATH, "wb") as f:
            pickle.dump(
                {
                    "ext_path_to_extinfo_map": global_ext_path_to_extinfo_map,
                    "ext_path_to_dep_paths_map": global_ext_path_to_dep_paths_map,
                },
                f,
            )
        logger.info(f"Cache saved to {LOCAL_CACHE_DB_PATH}")
        # print(f"global_ext_path_to_dep_paths_map: {global_ext_path_to_dep_paths_map}")
    except Exception as e:
        logger.error(f"Failed to save cache to {LOCAL_CACHE_DB_PATH}: {e}")

    # Unlock the cache db lock file
    fcntl.flock(LOCAL_CACHE_DB_LOCK_FILE, fcntl.LOCK_UN)
    LOCAL_CACHE_DB_LOCK_FILE.close()
    LOCAL_CACHE_DB_LOCK_FILE = None


def parse_list(value) -> list:
    if isinstance(value, str):
        # Split by comma or semicolon, then strip whitespace
        return [item.strip() for item in value.replace(";", ",").split(",") if item.strip()]
    elif isinstance(value, list):
        return value
    return []


def parse_dependencies(value) -> list:
    if isinstance(value, str):
        dependencies = []
        for dep_str in value.replace(";", ",").split(","):
            dep_dict = {}
            for pair in dep_str.strip().split():
                key, val = pair.split(":", 1)
                dep_dict[key.strip()] = val.strip()
            if dep_dict:
                dependencies.append(dep_dict)
        return dependencies
    elif isinstance(value, list):
        return value
    return []


def load_extensions(context, ext_path, loaded_ext_path_set, loaded_ext_uuid_set) -> None:
    """
    Load an extension into a GXF context, optionally traversing a provided dependency graph
    to pre-load known extension dependencies.
    """
    global global_ext_path_to_dep_paths_map, global_ext_path_to_extinfo_map
    ext_path = abspath(ext_path)

    # Check if the extension is already loaded
    if ext_path in loaded_ext_path_set:
        logger.info(f"Extension {ext_path} already loaded. Skipping.")
        return

    # Load the extension recursively
    logger.info(f"Loading extension '{ext_path}'")
    dependencies = global_ext_path_to_dep_paths_map.get(ext_path, [])
    logger.info(f"Dependencies of {ext_path}: {dependencies}")

    for dep_path in dependencies:
        try:
            load_extensions(context, dep_path, loaded_ext_path_set, loaded_ext_uuid_set)
        except ValueError as e:  # noqa: PERF203
            logger.warning(
                f"Extension {ext_path} failed to load dependency extension {dep_path}: {e}"
            )

    logger.info(f"Loading extension {ext_path}")

    # Cache identifying information about the loaded extension
    try:
        gxf.core.load_extensions(context=context, extension_filenames=[ext_path])
        loaded_ext_path_set.add(ext_path)
        ext_list = gxf.core.get_extension_list(context)
        # Identify the loaded extension uuid by comparing with loaded_ext_uuid_set
        for uuid in ext_list:
            if uuid not in loaded_ext_uuid_set:
                loaded_ext_uuid_set.add(uuid)
                ext_info = gxf.core.get_extension_info(context, uuid)
                logger.info(
                    f"Loaded extension {ext_info['name']} with uuid {uuid} and version "
                    f"{ext_info['version']} from '{ext_path}'"
                )
                global_ext_path_to_extinfo_map[ext_path] = (
                    ext_info["name"],
                    uuid,
                    ext_info["version"],
                )
    except ValueError as e:
        logger.error(f"Failed to load extension {ext_path}: {e}")


def generate_extension_dependencies(arg_dict: dict) -> list[dict]:
    """
    Generate a list of extension dependencies with version information for a given extension.

    :param arg_dict: Argument dictionary with expected keys:
    - extension_library: Path to the extension library under consideration.
    - search_paths: List of paths to search for extension libraries.
    - extensions_to_preload: Ordered list of extension filepaths to pre-load in the GXF context
        without including them explicitly in the dependency graph.
    - extension_dependencies: Ordered list of known direct extension dependencies.
    :returns: An ordered list of dictionaries containing a name, uuid, and version for each
        direct extension dependency.
    :throws: FileNotFoundError if an extension library cannot be found on the system.
    :throws: ValueError if an extension dependency fails to load in the GXF context,
        typically due to some missing or out-of-order transitive dependency.
    """
    global global_ext_path_to_dep_paths_map

    # List of (extension name, extension uuid, extension version) tuples to be returned
    ext_uuid_name_list = []

    ext_path = arg_dict["extension_library"]
    search_paths = arg_dict["search_path"]
    extensions_to_preload = arg_dict["extensions_preload"] or []
    extension_dependencies = arg_dict["extension_dependencies"] or []

    logger.info(f"Generating extension dependencies for {extension_dependencies}")

    # Convert extension_dependencies to a list if it's a string
    if isinstance(extension_dependencies, str):
        extension_dependencies = extension_dependencies.replace(";", ",").split(",")
    # Remove any empty strings from the list
    extension_dependencies = [dep for dep in extension_dependencies if dep.strip()]

    # If extension_dependencies is empty, return an empty dictionary
    if not extension_dependencies:
        return {}

    # Set of loaded extension paths
    loaded_ext_path_set = set()
    # Set of loaded extension uuids
    loaded_ext_uuid_set = set()

    # List of dependency paths
    dep_ext_paths = []

    def find_absolute_path(file: str, search_paths: list):
        """Find a requested extension on the local system"""
        if os.path.isabs(file):
            return file
        for search_path in search_paths:
            potential_path = os.path.join(search_path, file)
            if os.path.exists(potential_path):
                return potential_path
        raise FileNotFoundError(f"Could not find {file} on the system")

    preload_extension_filenames = [
        find_absolute_path(gxf_component, search_paths) for gxf_component in extensions_to_preload
    ]
    dep_extension_filenames = [
        find_absolute_path(gxf_component, search_paths) for gxf_component in extension_dependencies
    ]

    # Create a context
    context = gxf.core.context_create()

    # Pre-populate the context with specified dependencies and initialize cache entries
    gxf.core.load_extensions(context=context, extension_filenames=preload_extension_filenames)
    preloaded_ext_deps = {ext_path: [] for ext_path in preload_extension_filenames}
    preloaded_ext_deps.update(global_ext_path_to_dep_paths_map)
    global_ext_path_to_dep_paths_map = preloaded_ext_deps

    # Iteratively load specified extension dependencies
    for extension_filepath in dep_extension_filenames:
        # Add the extension path to the set of dependency paths
        dep_ext_paths.append(extension_filepath)

        # Load the dependent extensions recursively
        load_extensions(
            context=context,
            ext_path=extension_filepath,
            loaded_ext_path_set=loaded_ext_path_set,
            loaded_ext_uuid_set=loaded_ext_uuid_set,
        )

    # Store the dependency paths for the extension
    global_ext_path_to_dep_paths_map[ext_path] = dep_extension_filenames

    # For each dependency, get the extension name, uuid, and version
    for dep_ext_path in dep_ext_paths:
        if dep_ext_path in global_ext_path_to_extinfo_map:
            ext_name, ext_uuid, ext_version = global_ext_path_to_extinfo_map[dep_ext_path]
            ext_uuid_name_list.append(
                {
                    "extension": ext_name,
                    "uuid": ext_uuid,
                    "version": ext_version,
                }
            )
        else:
            raise ValueError(f"Extension {dep_ext_path} not found in the cache")

    # Destroy the context
    gxf.core.context_destroy(context)

    return ext_uuid_name_list


def generate_manifest_content(arg_dict) -> dict:
    """Generate data to populate a GXF extension manifest file."""
    global global_ext_path_to_extinfo_map
    manifest_data = DEFAULT_HOLOSCAN_MANIFEST_DATA.copy()
    for key, value in arg_dict.items():
        if value is not None:
            if key in [
                "labels",
                "badges",
                "headers",
                "binaries",
                "python_bindings",
                "python_sources",
                "data",
            ]:
                manifest_data[key] = parse_list(value)
            elif key == "dependencies":
                manifest_data[key] = parse_dependencies(value)

                ext_uuid_name_dict = None
                # Generate extension dependencies
                if arg_dict.get("extension_dependencies"):
                    ext_uuid_name_dict = generate_extension_dependencies(arg_dict)

                # If ext_uuid_name_dict not empty, replace the dependencies in arg_dict
                if ext_uuid_name_dict:
                    logger.info(f"replacing dependencies with {ext_uuid_name_dict}")
                    manifest_data[key] = ext_uuid_name_dict

            else:
                manifest_data[key] = value
        else:
            manifest_data[key] = None

    # Store uuid to extension info map
    ext_path = abspath(manifest_data["extension_library"])
    ext_name = manifest_data["name"]
    ext_uuid = manifest_data["uuid"]
    ext_version = manifest_data["version"]
    global_ext_path_to_extinfo_map[ext_path] = (ext_name, ext_uuid, ext_version)

    return manifest_data


def convert_to_yaml_filler(manifest_data) -> dict:
    yaml_filler = {}
    for key, value in manifest_data.items():
        if value is None:
            yaml_filler[key] = ""
        elif isinstance(value, list):
            if key == "dependencies":
                deps_yaml = yaml.dump(value, default_flow_style=False)
                yaml_filler[key] = "\n" + deps_yaml.rstrip() if value else " []"
            else:
                yaml_filler[key] = (
                    "\n" + "\n".join(f"- {item}" for item in value) if value else " []"
                )
        else:
            yaml_filler[key] = f" {value}"

    return yaml_filler


def write_manifest_file(content, output_file) -> None:
    with open(output_file, "w") as f:
        f.write(content)
    logger.info(f"Manifest file generated: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Generates a GXF manifest file for an extension."
        "The manifest file can be used to register the extension with the GXF registry."
        "Refer to Graph Composer documentation for more information on the GXF registry.\n"
        "https://docs.nvidia.com/metropolis/deepstream/dev-guide/graphtools-docs/docs/text/GraphComposer_Registry.html"
    )

    # Required arguments
    parser.add_argument("--name", required=True, help="Name of the extension")
    parser.add_argument(
        "--extension-library",
        required=True,
        help="Path of the extension library",
    )
    parser.add_argument("--uuid", required=True, help="UUID of the extension")
    parser.add_argument("--version", required=True, help="Version of the extension")
    parser.add_argument(
        "--license-file",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["license_file"],
        help="Path of the license file",
    )
    parser.add_argument(
        "--url",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["url"],
        help="URL of the extension",
    )
    parser.add_argument(
        "--git-repository",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["git_repository"],
        help="Git repository of the extension",
    )
    parser.add_argument(
        "--labels",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["labels"],
        help="List of labels (comma or semicolon-separated)",
    )
    parser.add_argument(
        "--badges",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["badges"],
        help="List of badges (comma or semicolon-separated)",
    )
    parser.add_argument(
        "--priority",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["priority"],
        help="Priority of the extension",
    )
    parser.add_argument(
        "--arch",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["arch"],
        help="Architecture of the extension",
    )
    parser.add_argument(
        "--os",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["os"],
        help="Operating system of the extension",
    )
    parser.add_argument(
        "--distribution",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["distribution"],
        help="Distribution of the extension",
    )
    parser.add_argument(
        "--cuda",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["cuda"],
        help="CUDA version of the extension",
    )
    parser.add_argument(
        "--tensorrt",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["tensorrt"],
        help="TensorRT version of the extension",
    )
    parser.add_argument(
        "--cudnn",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["cudnn"],
        help="cuDNN version of the extension",
    )
    parser.add_argument(
        "--deepstream",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["deepstream"],
        help="DeepStream version of the extension",
    )
    parser.add_argument(
        "--triton",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["triton"],
        help="Triton version of the extension",
    )
    parser.add_argument(
        "--vpi",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["vpi"],
        help="VPI version of the extension",
    )
    parser.add_argument(
        "--dependencies",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["dependencies"],
        help=(
            "List of dependencies (comma or semicolon-separated, 'key:value' pairs in each"
            " dependency string separated by spaces)."
            " Refer to Graph Composer documentation for more information on the"
            " dependency format."
        ),
    )
    parser.add_argument(
        "--headers",
        required=False,
        nargs="+",
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["headers"],
        help="List of header files",
    )
    parser.add_argument(
        "--binaries",
        required=False,
        nargs="+",
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["binaries"],
        help="List of binary files",
    )
    parser.add_argument(
        "--python-alias",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["python_alias"],
        help="Python alias of the extension",
    )
    parser.add_argument(
        "--namespace",
        required=False,
        default=DEFAULT_HOLOSCAN_NAMESPACE,
        help="Namespace of the extension",
    )
    parser.add_argument(
        "--python-bindings",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["python_bindings"],
        help="List of Python bindings (comma or semicolon-separated)",
    )
    parser.add_argument(
        "--python-sources",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["python_sources"],
        help="List of Python sources (comma or semicolon-separated)",
    )
    parser.add_argument(
        "--data",
        required=False,
        default=DEFAULT_HOLOSCAN_MANIFEST_DATA["data"],
        help="List of data files (comma or semicolon-separated)",
    )
    parser.add_argument(
        "--extension-dependencies",
        nargs="+",
        required=False,
        help="Ordered list of extension dependency filepaths (comma or semicolon-separated)."
        " Filepaths must exist in the local environment.",
    )
    # Optional arguments to assist in collecting extension dependencies
    parser.add_argument(
        "--search-path",
        nargs="+",
        required=False,
        help="Root path to search for extensions",
    )
    parser.add_argument(
        "--extensions-preload",
        nargs="+",
        required=False,
        help="Ordered list of extensions to pre-load in the GXF context before generating the"
        " manifest. Can be used to load indirect transitive dependencies without adding them"
        " to the manifest.",
    )
    parser.add_argument(
        "--db",
        required=False,
        help="Path to a database `.pickle` file with cached extension dependency graph information"
        " from previous runs of this script. "
        " Knowledge of transitive dependencies is required to load and query a GXF context."
        " If provided, the database will be loaded and saved after generating the manifest."
        " The database file will be created if it does not exist.",
    )
    # Add output file argument
    parser.add_argument("--output", required=True, help="Output manifest file path")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress logging output")

    args = parser.parse_args()

    if args.quiet:
        logger.setLevel(logging.WARNING)

    # Load cache db and register save_cache_db to be called when the program exits
    if args.db:
        global LOCAL_CACHE_DB_PATH
        global LOCAL_CACHE_DB_LOCK_PATH
        LOCAL_CACHE_DB_PATH = abspath(args.db)
        LOCAL_CACHE_DB_LOCK_PATH = abspath(args.db + ".lock")
    load_cache_db()
    atexit.register(save_cache_db)

    arg_dict = vars(args).copy()

    manifest_content = generate_manifest_content(arg_dict)

    # Remove the unnecessary arguments from the manifest_content
    for key in [
        "output",
        "search_path",
        "extension_dependencies",
        "extensions_preload",
    ]:
        del manifest_content[key]

    yaml_filler = convert_to_yaml_filler(manifest_content)
    final_manifest = MANIFEST_TEMPLATE.format(**yaml_filler)

    write_manifest_file(final_manifest, args.output)


if __name__ == "__main__":
    main()
